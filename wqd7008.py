# -*- coding: utf-8 -*-
"""WQD7008.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cgdq-PFV4oYGYN6dX0sz52MMvP5cyvRG
"""

# Install required packages
!pip install python-dotenv
!pip install apache-airflow
!pip install pyngrok

# Set up Airflow environment
from airflow import DAG
from datetime import datetime, timedelta
from airflow.operators.python_operator import PythonOperator

# Define default_args dictionary
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2022, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Create a DAG
dag = DAG(
    'weather_workflow',
    default_args=default_args,
    description='Weather Workflow',
    schedule_interval=timedelta(days=1),
)

# Define Python functions with parameters
def data_exploration(**kwargs):
    print("Step 1: Data Exploration")
    # Load your dataset and perform data exploration
    df = pd.read_csv("hourly_weather.csv")
    print(df.info())
    print(df.head())
    return df  # Return df for the next task

def data_preprocessing(df, **kwargs):
    print("Step 2: Data Preprocessing")
    df['date'] = pd.to_datetime(df['date'])
    df.set_index('date', inplace=True)
    print(df.index.min(), df.index.max())
    print(df.index.freq)
    print(df.isnull().sum())
    df.fillna(method='ffill', inplace=True)
    return df  # Return df for the next task

def feature_engineering(df, **kwargs):
    print("Step 3: Feature Engineering")
    df['hour'] = df.index.hour
    df['day'] = df.index.day
    df['month'] = df.index.month
    df['year'] = df.index.year
    lag_variables = ['temperature_2m', 'wind_speed_10m', 'precipitation', 'pressure_msl']
    for var in lag_variables:
        for i in range(1, 6):
            df[f'{var}_lag{i}'] = df[var].shift(i)
    return df  # Return df for the next task

def data_splitting(df, **kwargs):
    print("Step 4: Data Splitting")
    features = df.drop('weather_code', axis=1)
    target = df['weather_code']
    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)
    print("Training set shape:", X_train.shape, y_train.shape)
    print("Testing set shape:", X_test.shape, y_test.shape)
    return X_train, X_test, y_train, y_test  # Return values for the next task

def model_selection(df, **kwargs):
    print("Step 5: Model Selection")
    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
    imputer = SimpleImputer(strategy='mean')
    return rf_model, imputer  # Return values for the next task

def model_training(X_train, y_train, X_test, y_test, rf_model, imputer, **kwargs):
    print("Step 6: Model Training")
    X_train_imputed = imputer.fit_transform(X_train)
    X_test_imputed = imputer.transform(X_test)
    rf_model.fit(X_train_imputed, y_train)
    y_pred_imputed = rf_model.predict(X_test_imputed)
    return y_test, y_pred_imputed  # Return values for the next task

def evaluate_model_imputed(y_test, y_pred_imputed, **kwargs):
    print("Step 7: Evaluate Model on Imputed Data")
    accuracy_imputed = accuracy_score(y_test, y_pred_imputed)
    classification_report_result_imputed = classification_report(y_test, y_pred_imputed)
    conf_matrix_imputed = confusion_matrix(y_test, y_pred_imputed)
    print("Accuracy on Imputed Data:", accuracy_imputed)
    print("\nClassification Report on Imputed Data:\n", classification_report_result_imputed)
    print("\nConfusion Matrix on Imputed Data:\n", conf_matrix_imputed)

# Continue defining other functions...

# Define more tasks
task1 = PythonOperator(
    task_id='data_exploration',
    python_callable=data_exploration,
    provide_context=True,  # This ensures that the function receives the context (including task_instance)
    dag=dag,
)

task2 = PythonOperator(
    task_id='data_preprocessing',
    python_callable=data_preprocessing,
    provide_context=True,
    dag=dag,
)

task3 = PythonOperator(
    task_id='feature_engineering',
    python_callable=feature_engineering,
    provide_context=True,
    dag=dag,
)

task4 = PythonOperator(
    task_id='data_splitting',
    python_callable=data_splitting,
    provide_context=True,
    dag=dag,
)

task5 = PythonOperator(
    task_id='model_selection',
    python_callable=model_selection,
    provide_context=True,
    dag=dag,
)

task6 = PythonOperator(
    task_id='model_training',
    python_callable=model_training,
    provide_context=True,
    dag=dag,
)

task7 = PythonOperator(
    task_id='evaluate_model_imputed',
    python_callable=evaluate_model_imputed,
    provide_context=True,
    dag=dag,
)

# Set task dependencies
task1 >> task2 >> task3 >> task4 >> task5 >> task6 >> task7